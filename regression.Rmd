# STATS 101C - Regression Final Project

This file will be used to host all our code, visualizations, and text for the final report. I (Junwon) will move everything over to a document + R script before we submit, so don't worry about the formatting here!

Make sure to Slack the group before working on your code and Slack again after pushing changes!

## Introduction: context and background info. - Tiffany

o Cite any external sources\
o Can mention what variables you may believe to be associated with the response\
variable based on background information.\
o Approximately 100 words. Minimum length 80 words.

## Exploratory Data Analysis - Junwon, Han

o Explore potential relationships between the variables.\
o Must include graphics showing relationships\
o Recommended: Transformations of some variables\
o Recommended: Converting some numeric variables into categorical variables\
o Recommended: Exploration of possible interactions between variables\
o Minimum 8 data visualizations. Maximum of 20. All graphs/visualizations must be\
accompanied by a description of its significance. Descriptions must be a minimum of 20\
words, recommended something around 50 words.

```{r}
# Load packages
library(tidyverse)
library(tidymodels)
library(tidyselect)
library(recipes)
```

```{r}
# Load train and test data
train <- read_csv('train.csv')
test <- read_csv('test.csv')
```

```{r}
# Remove `name` column from train
train <- subset(train, select = -name)
```

## Preprocessing / Recipes - Bobo, Samantha

o If you use recipes or perform preprocessing of variables, you must explain the steps you\
performed and the reasoning behind them.\
o Length will vary depending on preprocessing steps. \~ 100-500 words seems reasonable.

```{r}
train <- train %>%
  mutate(across(where(is.numeric), 
                ~ ifelse(. < quantile(., 0.04, na.rm = TRUE), quantile(., 0.04, na.rm = TRUE), 
                         ifelse(. > quantile(., 0.96, na.rm = TRUE), quantile(., 0.96, na.rm = TRUE), .))))





# Skewed variable names
skewed_vars <- all_of(c('x0041e', 'x0042e', 'x0048e', 'x0050e', 'x0052e', 'x0053e', 'x0055e', 'x0056e', 'x0068e', 'x0074e', 'x0081e'))

# Variables with high percentage of zeros
high_zero_vars <- all_of(c('x0040e', 'x0041e', 'x0042e', 'x0043e', 'x0045e', 'x0046e', 'x0047e', 'x0048e', 'x0049e', 'x0050e', 'x0051e', 'x0052e', 'x0053e', 'x0054e', 'x0055e', 'x0056e'))

rf_recipe_overfit <- recipe(percent_dem ~ ., data = train) %>%
  step_rm(id) %>%
  step_zv(all_predictors()) %>%
  #step_impute_median(all_predictors(), -all_outcomes()) %>% 
  step_impute_knn(all_predictors()) %>%
  step_YeoJohnson(skewed_vars) %>%
  step_nzv(high_zero_vars, freq_cut = 65/35, unique_cut = 10) %>% 
  step_corr(all_predictors(), threshold = 0.70) %>%
  step_normalize(all_numeric_predictors(), -all_outcomes())

rf_recipe <- recipe(percent_dem ~ ., data = train) %>%
  step_rm(id) %>%
  step_zv(all_predictors()) %>%
  step_impute_knn(all_predictors()) %>%
  step_log(skewed_vars, base = exp(1), offset = 1) %>%
  step_nzv(high_zero_vars, freq_cut = 65/35, unique_cut = 10) %>%
  step_corr(all_predictors(), threshold = 0.70) %>%
  step_normalize(all_numeric_predictors(), -all_outcomes()) 
# Prep and bake the recipe


rf_recipe_prep <- prep(rf_recipe, training = train)
train_baked <- bake(rf_recipe_prep, new_data = train)
```

Here are the Data Preprocessing Steps we took: 1. Identifying Variables: o Skewed Variables: looked at the distributions of all predictors and identified highly skewed distributions o High Zero Variables: looked at the training data steps and counted how many zeros occurred on each column

2.  Removing ID `step_rm(id)` removes the `id` column from the dataset. IDs are unique to each observation and do not provide useful information for modeling.

3.  Removing Zero Variance Predictors `step_zv(all_predictors())` removes predictors that have zero variance as they don't help in making predictions and may lead to model overfitting.

4.  K-nearest Neighbors Imputation `step_impute_knn` Unlike median imputation, which fills missing values with the median of the column, k-NN imputation is more dynamic and can offer a better approximation for missing data by considering the feature similarities.

5.  Yeo-Johnson Transformation `step_YeoJohnson` Skewed variables are transformed using the Yeo-Johnson transformation to approximate a normal distribution. This makes the model more robust to outliers.

6.  Removing Near-Zero Variance Predictors `step_nrv` Variables with near-zero variance are removed. These are variables that contain values with a frequency that is either too high or too low to be informative. The parameters freq_cut and unique_cut are set to customize the filtering process.

7.  Correlation Filtering `step_corr` Correlated predictors (correlation above 0.7) are removed. High correlation among predictors can result in multicollinearity, making the model unstable.

8.  Normalization `step_normalize` All numerical variables are normalized to have zero mean and unit variance. This is particularly useful for models sensitive to the scale of input variables.

## Candidate models / Model evaluation / tuning - Everyone

o This section will discuss the various candidate models that were attempted.\
o Minimum of 5 candidate models. Maximum of 12 candidate models.\
o A brief description should accompany each candidate model.\
o Include a table listing of all candidate models attempted. Columns should include

▪ Model identifier\
▪ Type of model (e.g. linear regression, knn, random forest)\
▪ Engine\
▪ Recipe used or listing of variables in the model\
▪ Hyperparameters

o Model evaluation and tuning

▪ Discuss the evaluation and comparison of the candidate models that were\
attempted.\
▪ Students should use v-fold cross validation to measure the performance of the\
candidate models\
▪ Tuning of hyperparameters\
▪ Include a table summarizing the performance of each model. Columns should include\
• Model identifier\
• Metric score (most likely rmse)\
• SE of metric (if applicable)\
▪ Include a plot (like autoplot) comparing the performance of the different models.

```{r}
set.seed(1)
train_folds <- train %>% vfold_cv(v = 10, strata = 'percent_dem')
```


A random forest model with all columns except id as predictors

```{r}
rf_recipe_1 <- recipe(percent_dem ~., data = train) %>%
  step_rm(id) %>%
  step_impute_knn(all_predictors()) 

rf_model <- rand_forest(trees = 1000) %>% 
            set_engine("ranger", importance = "impurity")  %>% 
            set_mode("regression")

rf_workflow <- 
  workflow() %>%
  add_recipe(rf_recipe_1) %>%
  add_model(rf_model)

rf_crossval_fit_1 <- 
  rf_workflow %>%
  fit_resamples(train_folds)

rf_crossval_fit_1 %>% 
  collect_metrics()

```


A random forest model with important variables as predictors 

```{r}

## determine important variables
rf_fit <- 
  rf_workflow %>%
  fit(data = train)

fitted_rf_model <- rf_fit %>% extract_fit_parsnip()
imp <- data.frame(fitted_rf_model$fit$variable.importance)
not_imp_var <- rownames(top_n(imp, -40))

rf_recipe_2 <- recipe(percent_dem ~., data = train) %>%
  step_rm(id) %>%
  step_rm(all_of(not_imp_var)) %>%
  step_impute_knn(all_predictors()) 

## a random forest model excludes the 40 least important variables
rf_workflow_2 <- 
  workflow() %>%
  add_recipe(rf_recipe_2) %>%
  add_model(rf_model)

rf_crossval_fit_2 <- 
  rf_workflow_2 %>%
  fit_resamples(train_folds)

rf_crossval_fit_2 %>% 
  collect_metrics()
```


A random forest model with tuning of mtry
* the best mtry is 67 according to tuning
```{r}

#rf_grid <- expand.grid(mtry=seq(25, 123, 7))

#rf_model_3 <- rand_forest(trees = 1000, mtry = tune()) %>% 
#            set_engine("ranger")  %>% 
 #           set_mode("regression")

#rf_workflow_3 <- 
#  workflow() %>%
#  add_recipe(rf_recipe_1) %>%
#  add_model(rf_model_3)

#rf_res <- 
#  rf_workflow_3 %>% 
#  tune_grid(
#    resamples = train_folds,grid = rf_grid
#    )


rf_model_3 <- rand_forest(trees = 1000, mtry = 67) %>% 
            set_engine("ranger")  %>% 
            set_mode("regression")

rf_workflow_3 <- 
  workflow() %>%
  add_recipe(rf_recipe_2) %>%
  add_model(rf_model_3)

rf_crossval_fit_3 <- 
  rf_workflow_3 %>%
  fit_resamples(train_folds)

rf_crossval_fit_3 %>% 
  collect_metrics()


```
```{r}

rf_model_4 <- rand_forest(trees = 1000, mtry = 67) %>% 
            set_engine("ranger")  %>% 
            set_mode("regression")

rf_workflow_4 <- 
  workflow() %>%
  add_recipe(rf_recipe_1) %>%
  add_model(rf_model_3)

rf_crossval_fit_4 <- 
  rf_workflow_4 %>%
  fit_resamples(train_folds)

rf_crossval_fit_4 %>% 
  collect_metrics()

```

```{r}
rf_crossval_fit_4 %>% 
  collect_metrics()

```

```{r}


```{r}
test <- read_csv("test.csv")



rf_workflow_3 <- 
  workflow() %>%
  add_recipe(rf_recipe_2) %>%
  add_model(rf_model)


rf_fit_3 <- 
  rf_workflow_3 %>%
  fit(data = train)


```


```{r}
rf_test_res_3 <-
  rf_fit_3 %>%
  predict(test) %>%
  cbind(test %>% select(id))


rf_test_res <- 
  rf_test_res_3 %>%
  select(id = id, percent_dem = .pred)

write_csv(rf_test_res, "final_predictions_2.csv")
```




```


## Discussion of final model - Everyone

o Discuss the selection of the final model used for generating predictions.\
o Discussion of strengths and weaknesses of the model.\
o Possible improvements, including what additional data could be usef

