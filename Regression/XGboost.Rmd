```{r}
library(tidyverse)
library(recipes)
library(tidymodels)

```

```{r}
test <- read_csv("test.csv")
train <- read_csv('train.csv')
#train <- subset(train, select = c(-id, -name))
```


My notes for Samantha:

- 1) Dropping below 4th percentile and above 96th percentile on all numeric columns arbitrarily is bad. Deal with outliers differently.
- 2) total_votes should be dropped or dealt with somehow to avoid data leakage (can cause overfitting), or at the bare minimum its effects should be explored.
- 3) Ignore colinearity or deal with it differently (for Random Forrest)
- 4) Significant near duplicate columns or redundant columns (colinear yes but worse). Choose which ones to keep and which to simply drop.
- 5) Consider using CO1_001E - C01_027E to feature engineer education level columns (feature engineering).
- 6) Consider using gdp and income per capita column for 2016 to 2020 to feature engineer trend columns for increase or decrease from 2016-2020 (feature engineering).


```{r}
education_cols = c("c01_003e" , "c01_004e", "c01_005e","c01_006e" , "c01_007e", "c01_008e"  ,"c01_009e" , "c01_010e","c01_011e" ,"c01_012e" ,"c01_013e" , "c01_014e","c01_015e", "c01_016e", "c01_017e",                           "c01_018e" ,"c01_019e", "c01_020e" , "c01_021e", "c01_022e", "c01_023e", "c01_024e", "c01_025e", "c01_026e", "c01_027e")

cols_to_drop_1 = c("x0001e", "x0018e","x0019e", "x0020e", "x0021e", "x0022e", "x0023e","x0024e" , "x0025e", "x0026e", "x0027e", "x0029e", "x0030e", "x0031e",  "x0034e", "x0058e", "x0062e", "x0064e", "x0065e",   "x0066e", "x0067e", "x0068e", "x0069e", "x0076e" , "x0077e", "x0078e", "x0079e","x0080e","x0081e","x0082e","x0083e", "x0002e","x0003e","x0005e","x0006e","x0007e","x0008e","x0009e","x0010e","x0011e","x0012e","x0013e","x0014e","x0015e", "x0016e", "x0017e")
```


```{r}
#First aging scheme: Kept total columns: No education feature: No Colinearity Adjustment: No Normalization
code2013 <- c("one", "two", "three", "four", "five", "six")
## Create a recipe
rf_recipe_1 <- recipe(percent_dem ~ ., data = train) %>%
  step_rm(id) %>%
  step_impute_knn(all_predictors()) %>%
  step_mutate(pct_male = x0002e/x0001e) %>%
  step_mutate(pct_female = x0003e/x0001e)  %>%
  step_mutate(pct_male_18over = x0026e / x0001e) %>%
  step_mutate(pct_female_18over = x0027e / x0001e) %>%
  step_mutate(pct_21andover = x0022e/x0001e) %>%
  step_mutate(pct_62andove = x0023e/x0001e) %>%
  step_mutate(pct_female_65over = x0031e / x0001e) %>%
  step_mutate(pct_male_65over = x0030e / x0001e) %>%
  step_mutate(pct_16over = x0020e/ x0001e)%>%
  step_mutate(pct_18over = x0025e/ x0001e)%>%
  step_mutate(pct_0to19 = (x0005e + x0006e + x0007e + x0008e) / x0001e) %>%
  step_mutate(pct_20to34 = (x0009e + x0010e) / x0001e) %>%
  step_mutate(pct_35to54 = (x0011e + x0012e) / x0001e) %>%
  step_mutate(pct_55to64 = (x0013e + x0014e) / x0001e) %>%
  step_mutate(pct_65andovr = (x0015e + x0016e + x0017e) / x0001e) %>%
  step_mutate(pct_18o_citizen_m = x0088e / x0001e) %>%
  step_mutate(pct_18o_citizen_f = x0089e / x0001e) %>%
  step_mutate(pct_18o_citizen = x0087e / x0001e) %>%
  step_mutate(pct_onerace = x0034e / x0001e)  %>%
  step_mutate(pct_morerace = x0035e / x0001e) %>%
  step_mutate(pct_white = x0037e / x0001e) %>%
  step_mutate(pct_black = x0038e / x0001e) %>%
  step_mutate(pct_indian = x0039e / x0001e) %>%
  step_mutate(pct_asian = x0044e / x0001e) %>%
  step_mutate(pct_api = x0052e / x0001e) %>%
  step_mutate(pct_otherrace = x0057e / x0001e) %>%
  step_mutate(pct_white_c = x0064e / x0001e)  %>%
  step_mutate(pct_black_c = x0065e / x0001e)  %>%
  step_mutate(pct_indian_c = x0066e / x0001e)  %>%
  step_mutate(pct_asian_c = x0067e / x0001e)  %>%
  step_mutate(pct_api_c = x0068e / x0001e)  %>%
  step_mutate(pct_other_c = x0069e / x0001e)  %>%
  step_mutate(pct_his = x0071e / x0001e) %>%
  step_mutate(pct_cherokee = x0040e/x0001e) %>%
  step_mutate(pct_chippewa = x0041e/x0001e) %>%
  step_mutate(pct_navajo = x0042e/x0001e) %>%
  step_mutate(pct_sioux = x0043e/x0001e) %>%
  step_mutate(pct_indian = x0045e/x0001e) %>%
  step_mutate(pct_chinese = x0046e/x0001e) %>%
  step_mutate(pct_filipino = x0047e/x0001e) %>%
  step_mutate(pct_japanese = x0048e/x0001e) %>%
  step_mutate(pct_korean = x0049e/x0001e) %>%
  step_mutate(pct_viet = x0050e/x0001e) %>%
  step_mutate(pct_othera = x0051e/x0001e) %>%
  step_mutate(pct_hawaii = x0053e/x0001e) %>%
  step_mutate(pct_chamorro = x0054e/x0001e) %>%
  step_mutate(pct_samoan = x0055e/x0001e) %>%
  step_mutate(pct_otherpi = x0056e/x0001e) %>%
  step_mutate(pct_mexican = x0072e/x0001e) %>%
  step_mutate(pct_puertorican = x0073e/x0001e) %>%
  step_mutate(pct_cuban = x0074e/x0001e) %>%
  step_mutate(pct_otherh = x0075e/x0001e) %>%
  step_mutate(pct_whiteandblack = x0059e/x0001e) %>%
  step_mutate(pct_whiteandindian = x0060e/x0001e) %>%
  step_mutate(pct_whiteandasian = x0061e/x0001e) %>%
  step_mutate(pct_indianandblack = x0062e/x0001e) %>%
  step_mutate(pc_nothispanic_w = x0077e / x0001e) %>%
  step_mutate(pc_nothispanic_b = x0078e / x0001e) %>%
  step_mutate(pc_nothispanic_i = x0079e / x0001e) %>%
  step_mutate(pc_nothispanic_a = x0080e / x0001e) %>%
  step_mutate(pc_nothispanic_hpi = x0081e / x0001e) %>%
  step_mutate(pc_nothispanic_o = x0082e / x0001e) %>%
  step_mutate(pc_nothispanic_more = x0083e / x0001e) %>%
  step_mutate(pc_nothispanic_more_o = x0084e / x0001e) %>%
  step_mutate(pc_nothispanic_three = x0085e / x0001e) %>%
  step_mutate(pct_lesshighschool_18to24 = c01_002e / ifelse(c01_001e != 0, c01_001e, 1))  %>%
  step_mutate(pct_highschool_18to24 = c01_003e/ifelse(c01_001e != 0, c01_001e, 1)) %>%
  step_mutate(pc_college_18to24 = c01_004e/ifelse(c01_001e != 0, c01_001e, 1)) %>%
  step_mutate(pc_bachelor_18to24 = c01_005e/ifelse(c01_001e != 0, c01_001e, 1)) %>%
  step_mutate(pc_lesshighschool_25over = (c01_007e + c01_008e)/c01_006e) %>%
  step_mutate(pc_highschool_25over = c01_009e/c01_006e) %>%
  step_mutate(pc_college_25over = (c01_010e + c01_011e)/c01_006e) %>%
  step_mutate(pc_bachelorover_25over = c01_015e/c01_006e) %>%
  step_mutate(pc_highschoolover_25over = c01_014e/c01_006e) %>%
  step_mutate(pc_associate_25over = c01_011e / c01_006e ) %>%
  step_mutate(pc_bachlor_25over = c01_012e / c01_006e ) %>%
  step_mutate(pc_graduate_25over = c01_013e / c01_006e ) %>%
  step_mutate(pc_highschool_25to34 = c01_017e/ c01_016e) %>%
  step_mutate(pc_bachelor_25to34 = c01_018e/ c01_016e) %>%
  step_mutate(pc_highschool_35to44 = c01_020e/ c01_019e) %>%
  step_mutate(pc_bachelor_35to44 = c01_021e/ c01_019e) %>%
  step_mutate(pc_highschool_45to64 = c01_023e/ c01_022e) %>%
  step_mutate(pc_bachelor_45to64 = c01_024e/ c01_022e) %>%
  step_mutate(pc_highschool_65over = c01_026e/ c01_025e) %>%
  step_mutate(pc_bachelor_65over = c01_027e/ c01_025e) %>%
  step_mutate(mean_income_per_cap = (income_per_cap_2016 + income_per_cap_2017+ income_per_cap_2018 + income_per_cap_2019+ income_per_cap_2020) / 5)  %>%
  step_mutate(mean_gdp = (gdp_2016+ gdp_2017+ gdp_2018 + gdp_2019+ gdp_2020)/5) %>%
  step_mutate(pc_vote_population = total_votes / x0001e) %>%
  step_mutate(pc_vote_eligible = total_votes/x0087e) %>%
  #step_mutate(housing_unit_per_person = x0086e / x0001e)  %>%
  step_rm(x0002e, x0009e, x0010e, x0003e, x0005e, x0006e, x0007e, x0008e, x0033e, x0011e, x0012e, x0013e, x0014e, x0015e, x0016e, x0017e) %>%
  step_rm(x0020e, x0021e, x0022e, x0023e, x0024e, x0025e, x0029e, x0087e, x0088e, x0089e) %>% #age
  step_rm(x0034e, x0035e, x0036e, x0037e, x0038e, x0039e, x0040e, x0052e, x0044e,x0057e, x0058e, x0064e, x0065e,x0066e, x0067e, x0068e, x0069e, x0071e) %>% #race
  #step_rm(income_per_cap_2016, income_per_cap_2018, income_per_cap_2017, income_per_cap_2019, income_per_cap_2020) %>%
  #step_rm(gdp_2016, gdp_2017, gdp_2018, gdp_2019, gdp_2020) %>%
  step_rm(c01_002e, c01_015e,c01_011e,c01_010e, c01_009e, c01_008e, c01_007e,  c01_005e, c01_004e, c01_003e) %>% 
  step_rm(c01_017e, c01_018e, c01_019e, c01_020e, c01_021e, c01_022e, c01_023e, c01_024e, c01_025e, c01_026e, c01_027e)%>% 
  step_rm(x0041e, x0042e, x0043e, x0045e, x0046e, x0047e, x0048e, x0049e, x0050e, x0051e, x0053e, x0054e, x0055e, x0056e) %>% 
  step_rm(matches("^x007[2-9]e$|^x008[0-5]e$"))  %>%
  step_rm(x0019e, x0026e, x0027e, x0031e, x0030e, x0060e, x0061e, x0062e, x0059e) %>%
  step_rm(c01_001e, c01_006e, c01_012e, c01_013e, c01_016e)%>%
  step_rm(c01_014e)%>%
  step_rm(total_votes) %>%
  #step_rm(x0086e) %>%
 # step_corr(all_predictors(), threshold = 0.99) %>%
  step_num2factor(x2013_code, levels = code2013) %>%
  step_dummy(x2013_code) %>%
  #step_scale(all_predictors()) %>%
 # step_normalize(all_predictors()) %>%
  step_zv(all_predictors())


set.seed(10)
train_folds <- train %>% vfold_cv(v = 10, strata = 'percent_dem')
```

```{r}
#model
rf_model <- rand_forest(trees = 100) %>% 
            set_engine("ranger", importance = "impurity")  %>% 
            set_mode("regression")

library(xgboost)
# Create a random forest model specification
rf_model2 <-boost_tree(mode = "regression") %>%
  set_engine("xgboost")
rf_model2

catboost_spec <- boost_tree(
  mode = "regression",
  trees = 500,
  learn_rate = 0.009
) %>% 
  set_engine("lightgbm")

```

```{r}
library(bonsai)
#workflow
rf_workflow <- 
  workflow() %>%
  add_recipe(rf_recipe_1) %>%
  add_model(rf_model2)

rf_workflow2 <- 
  workflow() %>%
  add_recipe(rf_recipe_1) %>%
  add_model(catboost_spec)
```

```{r}
library(lightgbm)
rf_crossval_fit_1 <- 
  rf_workflow2 %>%
  fit_resamples(train_folds)

rf_crossval_fit_1 %>% 
  collect_metrics()
```
```{r}
#cross_validation definition
set.seed(1)
train_folds <- train %>% vfold_cv(v = 10, strata = 'percent_dem')
```

```{r}
rf_crossval_fit_1 <- 
  rf_workflow %>%
  fit_resamples(train_folds)

rf_crossval_fit_1 %>% 
  collect_metrics()
```

```{r}
# Define a more complex tuning grid
grid <- grid_regular(
  trees(range = c(100, 1000), trans = NULL),
  learn_rate(range = c(0.001, 0.1), trans = NULL),
  max_depth(range = c(1, 30), trans = NULL),
  min_child_weight(range = c(1, 10), trans = NULL),
  subsample(range = c(0.5, 1), trans = NULL),
  colsample_bytree(range = c(0.5, 1), trans = NULL),
  levels = 3  # or another number to control the grid size
)

```

```{r}
# Load the necessary libraries
library(tidymodels)
library(lightgbm)
library(dplyr)


# Define model specification
boost_tree_spec <- boost_tree(
  mode = "regression", 
  trees = 1000,
  learn_rate = 0.02575,
  # Define a tuning grid
  tree_depth = tune(),  # Adjust the range based on your knowledge of the problem
  mtry = tune(),  # Adjust the range based on your knowledge of the problem
  min_n = tune(),  # Adjust the range based on your knowledge of the problem
  loss_reduction = tune(),  # Adjust the range based on your 

) %>% set_engine("lightgbm", verbose = 1)

# Define a tuning grid
# Define a more complex tuning grid
# Define a tuning grid
grid <- grid_regular(
  tree_depth(range = c(1, 30), trans = NULL),  # Adjust the range based on your knowledge o
  mtry(range = c(2, 20), trans = NULL),  # Adjust the range based on your knowledge of the problem
  min_n(range = c(5, 50), trans = NULL),  # Adjust the range based on your knowledge of the problem
  loss_reduction(range = c(0, 0.1), trans = NULL),  # Adjust the range based on your knowledge of the problem
  levels = 5  # or another number to control the grid size
)




# Define a workflow
workflow <- workflow() %>% 
  add_model(boost_tree_spec) %>% 
  add_recipe(rf_recipe_1)

# Run the tuning grid
tune_res <- tune_grid(
  workflow,
  resamples = train_folds,
  grid = grid
)

# View results
show_best(tune_res, metric = "rmse")

```

