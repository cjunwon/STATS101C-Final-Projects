# STATS 101C - Regression Final Project

This file will be used to host all our code, visualizations, and text for the final report. I (Junwon) will move everything over to a document + R script before we submit, so don't worry about the formatting here!

Make sure to Slack the group before working on your code and Slack again after pushing changes!

## Introduction: context and background info. - Tiffany

o Cite any external sources\
o Can mention what variables you may believe to be associated with the response\
variable based on background information.\
o Approximately 100 words. Minimum length 80 words.

## Exploratory Data Analysis - Junwon, Han

o Explore potential relationships between the variables.\
o Must include graphics showing relationships\
o Recommended: Transformations of some variables\
o Recommended: Converting some numeric variables into categorical variables\
o Recommended: Exploration of possible interactions between variables\
o Minimum 8 data visualizations. Maximum of 20. All graphs/visualizations must be\
accompanied by a description of its significance. Descriptions must be a minimum of 20\
words, recommended something around 50 words.

```{r}
# Load packages
library(tidyverse)
library(tidymodels)
library(ggplot2)
```

```{r}
# Load train and test data
train <- read_csv('train.csv')
test <- read_csv('test.csv')
```

```{r}
str(train)
```

```{r}
# Remove `name` column from train
train <- subset(train, select = -name)
```

```{r}
ggplot(train, aes(x = as_factor(x2013_code), y = total_votes)) +
  geom_bar(stat = "identity")
```

```{r}
ggplot(train, aes(x = total_votes, y = percent_dem)) +
  geom_point() +
  facet_wrap(~x2013_code) +
  labs(x = 'Total Votes Cast', y = 'Percent voters who voted Biden', title = 'Biden voters vs Total votes by urban/rural code')
```

-   More variation in more urban areas

```{r}
ggplot(train, aes(x = income_per_cap_2020, y = percent_dem)) +
  geom_point() +
  facet_wrap(~x2013_code) +
  labs(x = 'income per capita for the county in 2020 (missing for some counties)', y = 'Percent voters who voted Biden', title = 'Biden voters vs Income per capita by urban/rural code')
```

-   Higher in urban, lower and clustered in rural

```{r}
ggplot(train, aes(x = c01_012e, y = income_per_cap_2020)) +
  geom_abline() +
  geom_point() +
  labs(x = 'Estimate:Total:Population 25 years and over:Bachelors degree', y = 'income per capita for the county in 2020 (missing for some counties)', title = '') +
  coord_obs_pred()
```

-   Seemingly no interaction between level of education & income

```{r}
ggplot(train, aes(x = total_votes, y = percent_dem)) +
  geom_abline() +
  geom_point() +
  labs(x = 'Total Votes Cast', y = 'Percent voters who voted Biden')
```

```{r}
ggplot(train, aes(x = gdp_2020, y = percent_dem)) +
  geom_abline() +
  geom_point() +
  labs(x = 'GDP for the county in 2020', y = 'Percent voters who voted Biden')
```

```{r}
ggplot(train, aes(x = x0009e, y = percent_dem)) +
  geom_abline() +
  geom_point() +
  labs(x = 'Estimate:Total population:20 to 24 years', y = 'Percent voters who voted Biden')
```

```{r}
ggplot(train, aes(x = x0014e, y = percent_dem)) +
  geom_abline() +
  geom_point() +
  labs(x = 'Estimate:Total population:60 to 64 years', y = 'Percent voters who voted Biden')
```

## Preprocessing / Recipes - Bobo, Samantha

o If you use recipes or perform preprocessing of variables, you must explain the steps you\
performed and the reasoning behind them.\
o Length will vary depending on preprocessing steps. \~ 100-500 words seems reasonable.

```{r}
library(tidyselect)
library(recipes)
library(tidyverse)
library(dplyr)

# Skewed variable names
skewed_vars <- all_of(c('x0041e', 'x0042e', 'x0048e', 'x0050e', 'x0052e', 'x0053e', 'x0055e', 'x0056e', 'x0068e', 'x0074e', 'x0081e'))

# Variables with high percentage of zeros
high_zero_vars <- all_of(c('x0040e', 'x0041e', 'x0042e', 'x0043e', 'x0045e', 'x0046e', 'x0047e', 'x0048e', 'x0049e', 'x0050e', 'x0051e', 'x0052e', 'x0053e', 'x0054e', 'x0055e', 'x0056e'))

rf_recipe <- recipe(percent_dem ~ ., data = train) %>%
  step_rm(id) %>%
  step_zv(all_predictors()) %>%
  step_impute_knn(all_predictors()) %>%
  step_YeoJohnson(skewed_vars) %>%
  step_nzv(high_zero_vars, freq_cut = 65/35, unique_cut = 10) %>% #remove near-zero variance predictors
  step_corr(all_predictors(), threshold = 0.70) %>%
  step_normalize(all_numeric_predictors(), -all_outcomes())
  
# Prep and bake the recipe
rf_recipe_prep <- prep(rf_recipe, training = train)
train_baked <- bake(rf_recipe_prep, new_data = train)

```

Here are the Data Preprocessing Steps we took: 
1. Identifying Variables: 
o Skewed Variables: looked at the distributions of all predictors and identified highly skewed distributions
o High Zero Variables: looked at the training data steps and counted how many zeros occurred on each column

2. Removing ID 
`step_rm(id)` removes the `id` column from the dataset. IDs are unique to each observation and do not provide useful information for modeling.

3. Removing Zero Variance Predictors 
`step_zv(all_predictors())` removes predictors that have zero variance as they don't help in making predictions and may lead to model overfitting.

4. K-nearest Neighbors Imputation
`step_impute_knn`  Unlike median imputation, which fills missing values with the median of the column, k-NN imputation is more dynamic and can offer a better approximation for missing data by considering the feature similarities.

6. Yeo-Johnson Transformation
`step_YeoJohnson` Skewed variables are transformed using the Yeo-Johnson transformation to approximate a normal distribution. This makes the model more robust to outliers.

7. Removing Near-Zero Variance Predictors
`step_nrv` Variables with near-zero variance are removed. These are variables that contain values with a frequency that is either too high or too low to be informative. 
The parameters freq_cut and unique_cut are set to customize the filtering process.

8. Correlation Filtering 
`step_corr` Correlated predictors (correlation above 0.7) are removed. High correlation among predictors can result in multicollinearity, making the model unstable.

9. Normalization 
`step_normalize` All numerical variables are normalized to have zero mean and unit variance. This is particularly useful for models sensitive to the scale of input variables.



## Candidate models / Model evaluation / tuning - Everyone

o This section will discuss the various candidate models that were attempted.\
o Minimum of 5 candidate models. Maximum of 12 candidate models.\
o A brief description should accompany each candidate model.\
o Include a table listing of all candidate models attempted. Columns should include

▪ Model identifier\
▪ Type of model (e.g. linear regression, knn, random forest)\
▪ Engine\
▪ Recipe used or listing of variables in the model\
▪ Hyperparameters

o Model evaluation and tuning

▪ Discuss the evaluation and comparison of the candidate models that were\
attempted.\
▪ Students should use v-fold cross validation to measure the performance of the\
candidate models\
▪ Tuning of hyperparameters\
▪ Include a table summarizing the performance of each model. Columns should include\
• Model identifier\
• Metric score (most likely rmse)\
• SE of metric (if applicable)\
▪ Include a plot (like autoplot) comparing the performance of the different models.

```{r}

```

## Discussion of final model - Everyone

o Discuss the selection of the final model used for generating predictions.\
o Discussion of strengths and weaknesses of the model.\
o Possible improvements, including what additional data could be usef
