```{r}
# Load packages
library(tidyverse)
library(tidymodels)
library(tidyselect)
library(recipes)
library(ranger)
library(xgboost)
library(keras)
```

```{r}
# Load train and test data
train <- read_csv('train2.csv', show_col_types = FALSE)
test <- read_csv('test2.csv', show_col_types = FALSE)
```

# EDA

```{r}
# create detect outlier function
detect_outlier <- function(x) {
 
    # calculate first quantile
    Quantile1 <- quantile(x, probs=.25)
 
    # calculate third quantile
    Quantile3 <- quantile(x, probs=.75)
 
    # calculate inter quartile range
    IQR = Quantile3-Quantile1
 
    # return true or false
    x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
 
# create remove outlier function
remove_outlier <- function(dataframe, columns=names(dataframe)) {
 
    # for loop to traverse in columns vector
    for (col in columns) {
        
        # Check if there are NA values in the column
        if (any(is.na(dataframe[[col]]))) {
            
            # Handle NA values (e.g., remove rows with NAs)
            dataframe <- dataframe[!is.na(dataframe[[col]]), ]
        }

        # Remove observation if it satisfies outlier function
        dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
    }
 
    return(dataframe)
}
 
# Create new dataframe with outliers removed for visualizations
train_outliers_rem <- remove_outlier(train, c('income'))
```

```{r}
# 
ggplot(train_outliers_rem, aes(x = loan_amount, 
                               y = income, 
                               col = as_factor(action_taken))) +
  geom_point(alpha = 0.5)
```

```{r}
bp_income <- ggplot(train_outliers_rem, aes(x = as_factor(action_taken), 
                                            y = income, 
                                            color = as_factor(action_taken))) +
  geom_boxplot()
  

bp_income
```

# Preprocessing / Recipes

```{r}
# Sort columns with and without NA
no_na_cols_count <- 
  train %>% 
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Column", values_to = "Missing_Count") %>%
  filter(Missing_Count == 0)

na_cols_count <- 
  train %>% 
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Column", values_to = "Missing_Count") %>%
  filter(Missing_Count > 0)

na_cols_count_high <- 
  train %>% 
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Column", values_to = "Missing_Count") %>%
  filter(Missing_Count > 300000)

no_na_cols <- pull(no_na_cols_count, Column)
na_cols <- pull(na_cols_count, Column)
na_cols_high <- pull(na_cols_count_high, Column)

# Change outcome variable into factor
train$action_taken <- as_factor(train$action_taken)

# Cross-validation data
set.seed(1)
train_folds <- vfold_cv(train, v = 10, strata = action_taken)
```

```{r}
no_na_cols
na_cols
na_cols_high
```

```{r}
# Candidate recipes

basic_recipe <- 
  recipe(action_taken ~ income, data = train)

# add more recipes here!
```

```{r}
# Choosing best recipe

# Initialize baseline recipe
log_spec <- 
  logistic_reg() %>%  
  set_engine(engine = "glm") %>% 
  set_mode("classification")

# Set up preprocess object containing all candidate recipes
preproc <- list(basic = basic_recipe
                # add more here
                )

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

# Create a workflow_set to test all candidate recipes against baseline logistic regression model
recipe_res <- workflow_set(preproc = preproc,
                          models = list(lr = log_spec),
                          cross = FALSE)

# Use workflow_map to call in v-fold cross validation data
recipe_res <- 
  recipe_res %>%
  workflow_map("fit_resamples",
               # Options to `workflow_map()`:
               seed = 1, verbose = TRUE,
               # Options to `fit_resamples()`:
               resamples = train_folds,
               metrics = metric_set(
                  recall, precision, f_meas, 
                  accuracy, kap,
                  roc_auc, sens, spec),
               control = keep_pred)
```

```{r}
# See which recipe performs best
recipe_res %>%
  collect_metrics() %>%
  filter(.metric == "f_meas")

autoplot(recipe_res)
```

# Candidate models / Model evaluation / tuning

```{r}
# Logistic regression
log_spec <- 
  logistic_reg() %>%  
  set_engine(engine = "glm") %>% 
  set_mode("classification")

# Random forest
rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

# Boosted tree (XGBoost)
xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbor
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% #
  set_engine("kknn") %>% 
  set_mode("classification") 

# Neural network - not tuning to save time
keras_spec <-
  mlp(epochs = 1000,
      hidden_units = 20,
      activation = "relu") %>%
  set_mode("classification") %>% 
  set_engine("keras", verbose = 0) 
```

```{r}
# Workflows

# Logistic regression workflow
log_wflow <-
 workflow() %>% 
 add_recipe(basic_recipe) %>%
 add_model(log_spec)

# Random forest workflow
rf_wflow <-
 workflow() %>%
 add_recipe(basic_recipe) %>% 
 add_model(rf_spec) 

# Boosted tree (XGBoost) workflow
xgb_wflow <-
 workflow() %>%
 add_recipe(basic_recipe) %>% 
 add_model(xgb_spec)

# K-nearest neighbor workflow
knn_wflow <-
 workflow() %>%
 add_recipe(basic_recipe) %>% 
 add_model(knn_spec)

# Neural network workflow
keras_wflow <-
 workflow() %>%
 add_recipe(basic_recipe) %>% 
 add_model(keras_spec)
```

```{r}
# Model evaluation
set.seed(1)

log_res <- 
  log_wflow %>%
  fit_resamples(resamples = train_folds,
                control = control_resamples(save_pred = TRUE))
```

```{r}
log_res %>%
  collect_metrics()
```
